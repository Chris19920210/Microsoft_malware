This project is mainly about Microsoft Malware Classification

The main goal is to efficiently classify the malware files according to their respective families.
Feature engineering: words are main features for the malware files, including bytes (compression in hexadecimal formMicrosoft Malware Classification Challenge
Background:
Classify malware into families based on file content and characteristics
In recent years, the malware industry has become  a well organized market involving large amounts of money, which requiring anti-malware vendors to develop mechanisms for locating and deactivating them.
One of the major challenges that ant-malware faces nowadays is the the data and files in exceeding large scale which need to be evaluated for potential malicious intent. In order to be effective in analyzing and classifying such large amount of files, we need to be able to group them into groups and identify their respective families.

Data description:
(Training set - 10869 observations; test set – 10874; Storage – 500+gigabytes) 
A set of known malware files representing a mix of 9 different families are provided. Each malware file has an Id, a 20 character hash value uniquely identifying the file, and a Class, an integer representing one of 9 family names to which the malware may belong: 1-- Ramnit, 2 -- Lollipop, 3 – Kelihos_ver3, 4 – Vundo, 5 – Simda, 6 – Tracur, 7 – Kelihos_ver1, 8 – Obfuscator.ACY, 9 – Gatak. 

For each file, the raw data contains the hexadecimal representation of the file's binary content, without the PE header (to ensure sterility). You are also provided a metadata manifest, which is a log containing various metadata information extracted from the binary, such as function calls, strings, etc. This was generated using the IDA disassembler tool. In other words, there are two different formats copies for each file: one with suffix ‘.bytes’ ; another with ‘.asm’

Feature engineering:
Single word count:
1. Frequency of hexadecimal characters for both raw data and metadata manifest, eg ‘03’, ‘FF’. (the main body for both raw data and metadata manifest are compressed in hexadecimal representation)
2. Frequency of operations for metadata manifest, eg ‘mov’ , ‘xor’. (metadata manifest includes functions call and string, etc)

2-gram word count (adjacent words)
1. Frequency of 2-gram hexadecimal characters for both raw data and metadata manifest, eg ‘(03, FF)’. (the combinations of adjacent words)
2. Frequency of 2-gram operation for metadata manifest, eg ‘(mov, eax)’. (function calls + arguments)
(The reason for considering only up to 2-gram is due to the limitation for extracting the feature by personal computer. Generally speaking, we can consider n-gram as long as extra computation resource is available)

Methods description:
Raw data cleaning:
Because  the dimension of 2-gram frequency for hexadecimal representation are extremely large, we firstly eliminate the features which has maximum counts less than 200 for convenience.

Variable Selection:
(Because of the unacceptable volumes (35,000+) for feature space, variable selection is badly needed.)

*Screening based on information criterion:
Sure independence screening and iterative sure independence screening are both considered in this case. However, they are both abandoned due to the unacceptable computation efforts.

Random forest:
Variable selection based on OOB errors by random forest are implemented in this case due to its  simplicity and fastness. Note that we choose 1500+ relatively important features out of 35000+.

Classification Model:
We consider the ensemble methods in this case because of the huge volume for the data  set. Specifically, gradient boosting is adopted in this case. Moreover, grid search cross validation is used for tuning the hyperparameters.

Details:
Grid:
number of rounds: 100, 200, 500
learning rate: 0.1, 0.5, 0.8
maximum depth for individual tree: 3, 6, 8
subsample(bootstrap sampling): 0.9, 1.
columns subsample(feature sampling): 0.9, 1.

Loss function:
log loss

Best parameters:
number of rounds:100
learning rate: 0.5
maximum depth for individual tree:6
subsample: 0.9
columns subsample: 1

Evaluation for models:
Based on training set (1/3 for test, 2/3 for training) 
Predict Accuracy: 0.994703094508
XGboost:
             precision    recall  f1-score   support

          1       0.99      1.00      1.00       520
          2       1.00      1.00      1.00       811
          3       1.00      1.00      1.00       998
          4       0.95      0.99      0.97       146
          5       1.00      0.91      0.95        11
          6       1.00      0.99      0.99       233
          7       0.99      0.99      0.99       139
          8       1.00      0.98      0.99       410
          9       0.99      0.99      0.99       319

avg / total       0.99      0.99      0.99      3587


Kaggle competition : Top 15% post deadline

Language:
Python
Module:
sklearn, xgboost, nltk
